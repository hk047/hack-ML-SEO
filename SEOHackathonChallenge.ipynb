{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hk047/hack-ML-SEO/blob/main/SEOHackathonChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "T-HS19cTpRry",
    "outputId": "a8bc857d-1bac-45f0-a8c5-63ffa9ee7066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting cmudict (from textstat)\n",
      "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.6.1)\n",
      "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyphen, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, vaderSentiment, nvidia-cusparse-cu12, nvidia-cudnn-cu12, cmudict, textstat, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed cmudict-1.0.32 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyphen-0.17.2 textstat-0.7.5 vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate tensorflow textstat vaderSentiment\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'YOUR_TOKEN' with your Hugging Face token\n",
    "login(token=userdata.get('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LjM3WG9FsIe"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load SpaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "def load_model():\n",
    "    model_name = \"google/flan-t5-xl\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "    print(\"Model and tokenizer loaded.\")\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6Fr8h5QZziy"
   },
   "source": [
    "# Output From ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "Kblz3YgOKyw-",
    "outputId": "65e792ce-e248-4958-848e-4b65720e92f0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bf634d42e34adbb8bb6dbf1d8bfb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3de0c9022a74081952a30569f8667ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c2f600364c437fb33076930c992991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be0eedbcfde4f29add136f51288c3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9913e9f12a9f4b30be2f04aaa25fd4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8152e15540cc42f2a8637f9f19fc089b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and tokenizer loaded successfully.\n",
      "Starting dataset generation...\n",
      "Sample Headline: [headline]\n",
      "Sample Excerpt: [article_body]\n",
      "Topic: Autonomous Vehicles\n",
      "- [Topic]\n",
      "- [Topic]\n",
      "- [Topic]\n",
      "- [Topic]\n",
      "- [Topic]\n",
      "- [Topic]\n",
      "- [Topic]\n",
      "- [Topic]\n",
      "- [Topic]\n",
      "- [Topic]\n",
      "- [Topic...\n",
      "Sample Headline: [headline]\n",
      "Sample Excerpt: [article_body]\n",
      "Topic: Artificial Intelligence (AI)\n",
      "Date: 2025\n",
      "Date: 2025\n",
      "Date: 2025\n",
      "Date: 2025\n",
      "Date: 2025\n",
      "Date: 2025\n",
      "Date: 2025\n",
      "Date: 2025\n",
      "Date: 2025\n",
      "...\n",
      "Sample Headline: [headline]\n",
      "Sample Excerpt: [article_body]\n",
      "Topic: 5G Technology\n",
      "Author: Your Name\n",
      "Date: [date]\n",
      "In 2025, we witnessed a revolutionary advancement in technology with the introducti...\n",
      "Sample Headline: [headline]\n",
      "Sample Excerpt: [article_body]\n",
      "Topic: Cloud Computing\n",
      "Company: Meta Services\n",
      "Product: Meta Cloud Computing Platform\n",
      "Dates: 2025\n",
      "Company: Meta Services\n",
      "Dates: 2025\n",
      "Pro...\n",
      "Sample Headline: [headline]\n",
      "Sample Excerpt: [article_body]\n",
      "Topic: 3D Printing\n",
      "- 2025 is the year when the technology will be widely adopted, with [funding round] events in [location].\n",
      "- [product...\n",
      "Sample Headline: [headline]\n",
      "Sample Excerpt: [article_body]\n",
      "Topic: Autonomous Vehicles\n",
      "Sub-Topic: 2025\n",
      "2025 headlines\n",
      "2025 article body\n",
      "2025 quotes\n",
      "2025 statistics\n",
      "2025 implications\n",
      "2025 future o...\n",
      "Sample Headline: [headline]\n",
      "Sample Excerpt: [article_body]\n",
      "Topic: Robotics\n",
      "Technology: Breakthrough Announcement\n",
      "Topic: Robotics\n",
      "Technology: Breakthrough Announcement\n",
      "Topic: Robotics\n",
      "Technology:...\n",
      "Sample Headline: [headline]\n",
      "Sample Excerpt: [article_body]\n",
      "Topic: Artificial Intelligence (AI)\n",
      "- [article_body]\n",
      "- [article_body]\n",
      "- [article_body]\n",
      "- [article_body]\n",
      "- [article_body]\n",
      "- [article_bod...\n",
      "Sample Headline: [headline]\n",
      "Sample Excerpt: [article_body]\n",
      "Topic: Blockchain\n",
      "- [author]\n",
      "- [date]\n",
      "- [headline]\n",
      "- [article_body]\n",
      "- [author]\n",
      "- [date]\n",
      "- [headline]\n",
      "- [article_body]\n",
      "- [author]\n",
      "- [dat...\n",
      "Generated 10/500 articles\n",
      "Sample Headline: [headline]\n",
      "Sample Excerpt: [article_body]\n",
      "Topic: Augmented/Virtual Reality (AR/VR)\n",
      "Topic: [topic]\n",
      "Topic: [topic]\n",
      "Topic: [topic]\n",
      "Topic: [topic]\n",
      "Topic: [topic]\n",
      "Topic: [topic]\n",
      "Topi...\n",
      "Dataset generation complete. Saved to 'dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Step 1: Load model and tokenizer\n",
    "def load_model():\n",
    "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "    print(\"✅ Model and tokenizer loaded successfully.\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# Step 2: Define parameters\n",
    "topics = [\"Artificial Intelligence (AI)\", \"Robotics\", \"Blockchain\", \"Augmented/Virtual Reality (AR/VR)\",\n",
    "          \"Cybersecurity\", \"Quantum Computing\", \"5G Technology\", \"Autonomous Vehicles\",\n",
    "          \"Cloud Computing\", \"3D Printing\"]\n",
    "sentiments = [\"positive\", \"negative\"]\n",
    "article_types = [\"breakthrough announcement\", \"market analysis\", \"ethical debate\", \"product launch\",\n",
    "                 \"funding round\", \"acquisition deal\", \"financial news\", \"research development\", \"opinion piece\"]\n",
    "\n",
    "# Name generation functions\n",
    "def generate_expert_name():\n",
    "    first_names = [\n",
    "        \"Dr. Alan\", \"Professor Maria\", \"Sarah\", \"John\", \"Dr. Emily\", \"Professor David\",\n",
    "        \"Laura\", \"Michael\", \"Dr. Chen\", \"Professor Lee\", \"Jessica\", \"Kevin\", \"Dr. Aisha\",\n",
    "        \"Professor Hiroshi\", \"Sophia\", \"Liam\", \"Dr. Fatima\", \"Professor Omar\", \"Nathan\",\n",
    "        \"Isabella\", \"Dr. Vikram\", \"Professor Elena\", \"James\", \"Charlotte\", \"Dr. Ananya\",\n",
    "        \"Professor Luca\", \"Olivia\", \"Ethan\", \"Dr. Ravi\", \"Professor Yuki\", \"Zoe\", \"Benjamin\",\n",
    "        \"Dr. Amir\", \"Professor Santiago\", \"Ella\", \"Daniel\", \"Dr. Mei\", \"Professor Kartik\", \"Noah\"\n",
    "    ]\n",
    "\n",
    "    last_names = [\n",
    "        \"Chen\", \"Patel\", \"Johnson\", \"Lee\", \"Gomez\", \"Smith\", \"Davis\", \"Brown\",\n",
    "        \"Taylor\", \"Moore\", \"Singh\", \"Martinez\", \"Kumar\", \"Schneider\", \"Okafor\",\n",
    "        \"Garcia\", \"Hernandez\", \"Rossi\", \"Takahashi\", \"Nguyen\", \"Kowalski\",\n",
    "        \"Hassan\", \"Dubois\", \"Kim\", \"Ivanov\", \"Chowdhury\", \"Silva\", \"Al-Farsi\",\n",
    "        \"Fernandez\", \"Yamamoto\", \"Lopez\", \"Wang\", \"O’Reilly\", \"Kravchenko\", \"Bakshi\", \"Andersson\"\n",
    "    ]\n",
    "\n",
    "    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
    "\n",
    "def generate_company_name():\n",
    "    prefixes = [\n",
    "        \"Tech\", \"Quantum\", \"Cyber\", \"Data\", \"Innovate\", \"Future\", \"Global\",\n",
    "        \"Secure\", \"Smart\", \"Digital\", \"NextGen\", \"AI\", \"Deep\", \"Neural\",\n",
    "        \"Hyper\", \"Green\", \"Nano\", \"Optima\", \"Fusion\", \"Eco\", \"Synergy\",\n",
    "        \"Velocity\", \"Evolve\", \"Apex\", \"Pioneer\", \"Vortex\", \"Envision\", \"Nimbus\",\n",
    "        \"Infini\", \"Astral\", \"Meta\", \"Genetic\", \"Cloud\", \"Neuro\", \"Titan\", \"Arti\", \"Horizon\"\n",
    "    ]\n",
    "\n",
    "    suffixes = [\n",
    "        \"Labs\", \"Corp\", \"Inc\", \"Technologies\", \"Group\", \"Solutions\",\n",
    "        \"Systems\", \"Network\", \"Services\", \"International\", \"Enterprises\",\n",
    "        \"AI\", \"Analytics\", \"Blockchain\", \"Security\", \"Innovations\", \"Research\",\n",
    "        \"Automations\", \"Industries\", \"Dynamics\", \"Synergies\", \"Computing\", \"Robotics\", \"Softworks\"\n",
    "    ]\n",
    "\n",
    "    return f\"{random.choice(prefixes)} {random.choice(suffixes)}\"\n",
    "\n",
    "# Step 3: Prompt template\n",
    "def create_prompt(topic, sentiment, article_type, expert_name, company_name):\n",
    "    return f\"\"\"\n",
    "You are a skilled journalist writing a professional news article on {topic} with a {sentiment} sentiment.\n",
    "The article type is a '{article_type}'. Follow these guidelines:\n",
    "- Write a headline (10-15 words) that grabs attention and reflects the {sentiment} sentiment.\n",
    "- Write an article body (250-500 words) with:\n",
    "  - An introductory paragraph summarizing a key event or development in 2025.\n",
    "  - Detailed paragraphs with fictional specifics (company: {company_name}, dates in 2025, product details).\n",
    "  - Include a quote from {expert_name}, a credible expert, reinforcing the {sentiment} sentiment.\n",
    "  - Add optional statistics (e.g., 'a 30% increase') for realism.\n",
    "  - End with a concluding paragraph on implications or future outlook.\n",
    "- Ensure the {sentiment} sentiment is clear through language, tone, and context.\n",
    "- Mimic the style of authentic technology journalism, avoiding real-world references except general trends.\n",
    "Output in this format:\n",
    "Headline: [headline]\n",
    "Article Body: [article_body]\n",
    "Topic: {topic}\n",
    "\"\"\"\n",
    "\n",
    "# Step 4: Article generation function\n",
    "def generate_article(model, tokenizer, prompt):\n",
    "    try:\n",
    "        # Move model to the correct device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # Tokenize and move inputs to the same device as model\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}  # Move tensors to the model's device\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=1000,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ Error generating article: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 5: Output parsing function\n",
    "def parse_response(text):\n",
    "    match = re.search(r\"Headline:\\s*(.*?)\\nArticle Body:\\s*(.*)\", text, re.DOTALL)\n",
    "    return match.groups() if match else None\n",
    "\n",
    "# Step 6: Generate dataset\n",
    "def generate_dataset():\n",
    "    tokenizer, model = load_model()\n",
    "    articles = []\n",
    "    article_counts = {topic: {sentiment: 0 for sentiment in sentiments} for topic in topics}\n",
    "    topic_sentiment_pairs = [(t, s) for t in topics for s in sentiments]\n",
    "    random.shuffle(topic_sentiment_pairs)\n",
    "\n",
    "    print(\"Starting dataset generation...\")\n",
    "\n",
    "    while len(articles) < 10:\n",
    "        for topic, sentiment in topic_sentiment_pairs:\n",
    "            if article_counts[topic][sentiment] < 25:\n",
    "                article_type = random.choice(article_types)\n",
    "                expert_name = generate_expert_name()\n",
    "                company_name = generate_company_name()\n",
    "                prompt = create_prompt(topic, sentiment, article_type, expert_name, company_name)\n",
    "                generated_text = generate_article(model, tokenizer, prompt)\n",
    "\n",
    "                if generated_text:\n",
    "                    parsed = parse_response(generated_text)\n",
    "                    if parsed:\n",
    "                        headline, article_body = parsed\n",
    "                        word_count = len(article_body.split())\n",
    "\n",
    "                        if 250 <= word_count <= 500:\n",
    "                            articles.append([headline, article_body, topic])\n",
    "                            article_counts[topic][sentiment] += 1\n",
    "\n",
    "                            # Log progress every 10 articles\n",
    "                            if len(articles) % 10 == 0:\n",
    "                                print(f\"Generated {len(articles)}/500 articles\")\n",
    "\n",
    "                            # Log a short preview of the article\n",
    "                            print(f\"Sample Headline: {headline}\")\n",
    "                            print(f\"Sample Excerpt: {article_body[:150]}...\")\n",
    "\n",
    "                            if len(articles) == 10:\n",
    "                                break\n",
    "        if len(articles) == 10:\n",
    "            break\n",
    "\n",
    "    # Save dataset\n",
    "    with open('dataset.csv', 'w', newline='', encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Headline', 'Article Body', 'Topic'])\n",
    "        writer.writerows(articles)\n",
    "\n",
    "    print(\"Dataset generation complete. Saved to 'dataset.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA0g2Hq7cQsw"
   },
   "source": [
    "# Grok3 Revised (llama3.2-1b-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "k0O12YlwcUeW",
    "outputId": "461c3e65-9952-48e0-d53a-c46be5dd8ab0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b27b8bda42342e5b64a79ea16f8e5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9193368976483c859b75c14d1f542c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20092ac4057b412091b44ed062970abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014752d6271f44ae802b073d68cbba41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4207915114f34b9ebc78c918276569d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55744c42747547f4ac32489c32235ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b0f7a342a84225834e8d45e78274a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid headline for Augmented/Virtual Reality (AR/VR). Skipping.\n",
      "Article for 5G Technology discarded (word count: 68).\n",
      "Failed to generate valid headline for Cloud Computing. Skipping.\n",
      "Article for Artificial Intelligence (AI) discarded (word count: 69).\n",
      "Failed to generate valid headline for Quantum Computing. Skipping.\n",
      "Failed to generate valid headline for 5G Technology. Skipping.\n",
      "Article for Blockchain discarded (word count: 72).\n",
      "Failed to generate valid headline for Cloud Computing. Skipping.\n",
      "Article for Cybersecurity discarded (word count: 70).\n",
      "Failed to generate valid headline for Blockchain. Skipping.\n",
      "Dataset generation complete.\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "def load_model():\n",
    "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Stick with this due to RAM constraints\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# Define parameters\n",
    "topics = [\"Artificial Intelligence (AI)\", \"Robotics\", \"Blockchain\", \"Augmented/Virtual Reality (AR/VR)\",\n",
    "          \"Cybersecurity\", \"Quantum Computing\", \"5G Technology\", \"Autonomous Vehicles\",\n",
    "          \"Cloud Computing\", \"3D Printing\"]\n",
    "sentiments = [\"positive\", \"negative\"]\n",
    "article_types = [\"breakthrough announcement\", \"market analysis\", \"ethical debate\", \"product launch\"]\n",
    "\n",
    "# Name generation functions\n",
    "def generate_expert_name():\n",
    "    first_names = [\"Dr. Alan\", \"Professor Maria\", \"Sarah\", \"John\", \"Dr. Emily\", \"Professor David\"]\n",
    "    last_names = [\"Chen\", \"Patel\", \"Johnson\", \"Lee\", \"Gomez\", \"Smith\"]\n",
    "    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
    "\n",
    "def generate_company_name():\n",
    "    prefixes = [\"Tech\", \"Quantum\", \"Cyber\", \"Data\", \"Innovate\", \"Future\"]\n",
    "    suffixes = [\"Labs\", \"Corp\", \"Inc\", \"Technologies\", \"Group\", \"Solutions\"]\n",
    "    return f\"{random.choice(prefixes)} {random.choice(suffixes)}\"\n",
    "\n",
    "# Generate headline\n",
    "def generate_headline(model, tokenizer, topic, sentiment, article_type):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prompt = f\"Write a {sentiment} headline (10-15 words) about {topic} for a {article_type} article in 2025.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=30,  # Short length for headline\n",
    "        num_beams=3,   # Beam search for coherence\n",
    "        no_repeat_ngram_size=2,  # Avoid repetition\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    headline = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    # Remove prompt leakage and ensure length\n",
    "    headline = headline.replace(prompt, \"\").strip()\n",
    "    words = headline.split()\n",
    "    return headline\n",
    "\n",
    "# Generate article body\n",
    "def generate_article_body(model, tokenizer, headline, topic, sentiment, article_type, expert_name, company_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prompt = f\"\"\"\n",
    "Write a {sentiment} {article_type} article (250-500 words) about {topic} in 2025.\n",
    "Headline: \"{headline}\"\n",
    "Include:\n",
    "- Intro: Key event in 2025.\n",
    "- Details: Use {company_name}, dates in 2025, product specifics.\n",
    "- Quote: From {expert_name}.\n",
    "- Conclusion: Future implications.\n",
    "Example: \"In 2025, Tech Labs unveiled an AI tool, Synthia, boosting efficiency by 30%. 'A game-changer,' said Dr. John Lee.\"\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=500,  # Cap for article body\n",
    "        num_beams=5,    # Higher beams for better structure\n",
    "        no_repeat_ngram_size=3,  # Reduce repetition\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    article_body = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    # Clean up prompt leakage\n",
    "    article_body = article_body.replace(prompt, \"\").strip()\n",
    "    return article_body\n",
    "\n",
    "# Generate dataset\n",
    "def generate_dataset(num_articles=10):\n",
    "    tokenizer, model = load_model()\n",
    "    articles = []\n",
    "    for _ in range(num_articles):\n",
    "        topic = random.choice(topics)\n",
    "        sentiment = random.choice(sentiments)\n",
    "        article_type = random.choice(article_types)\n",
    "        expert_name = generate_expert_name()\n",
    "        company_name = generate_company_name()\n",
    "\n",
    "        # Generate headline with retry mechanism\n",
    "        for _ in range(3):  # Try up to 3 times\n",
    "            headline = generate_headline(model, tokenizer, topic, sentiment, article_type)\n",
    "            if headline:\n",
    "                break\n",
    "        if not headline:\n",
    "            print(f\"Failed to generate valid headline for {topic}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Generate article body\n",
    "        article_body = generate_article_body(model, tokenizer, headline, topic, sentiment, article_type, expert_name, company_name)\n",
    "\n",
    "        # Validate word count\n",
    "        word_count = len(article_body.split())\n",
    "        if 250 <= word_count <= 500:\n",
    "            articles.append([headline, article_body, topic])\n",
    "            print(f\"Generated article for {topic} with {sentiment} sentiment.\")\n",
    "        else:\n",
    "            print(f\"Article for {topic} discarded (word count: {word_count}).\")\n",
    "\n",
    "    # Save to CSV\n",
    "    with open('dataset-distilgpt2.csv', 'w', newline='', encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Headline', 'Article Body', 'Topic'])\n",
    "        writer.writerows(articles)\n",
    "    print(\"Dataset generation complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bjHgEw5hV3T"
   },
   "source": [
    "# Grok3 (gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aTcO9DehaMh",
    "outputId": "7f65cb5e-3bd6-4c5c-e55b-55dbcc0f8874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate headline for Artificial Intelligence (AI). Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated article for 3D Printing with positive sentiment.\n",
      "Generated article for Artificial Intelligence (AI) with positive sentiment.\n",
      "Failed to generate headline for Robotics. Skipping.\n",
      "Generated article for Augmented/Virtual Reality (AR/VR) with positive sentiment.\n",
      "Failed to generate valid article for 5G Technology. Skipping.\n",
      "Failed to generate headline for Robotics. Skipping.\n",
      "Generated article for Autonomous Vehicles with positive sentiment.\n",
      "Generated article for Autonomous Vehicles with positive sentiment.\n",
      "Generated article for Artificial Intelligence (AI) with negative sentiment.\n",
      "Dataset generation complete.\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    model_name = \"gpt2\"  # Using GPT-2 Small for better text generation\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# Define parameters\n",
    "topics = [\"Artificial Intelligence (AI)\", \"Robotics\", \"Blockchain\", \"Augmented/Virtual Reality (AR/VR)\",\n",
    "          \"Cybersecurity\", \"Quantum Computing\", \"5G Technology\", \"Autonomous Vehicles\",\n",
    "          \"Cloud Computing\", \"3D Printing\"]\n",
    "sentiments = [\"positive\", \"negative\"]\n",
    "article_types = [\"breakthrough announcement\", \"market analysis\", \"ethical debate\", \"product launch\"]\n",
    "\n",
    "# Name generation functions\n",
    "def generate_expert_name():\n",
    "    first_names = [\"Dr. Alan\", \"Professor Maria\", \"Sarah\", \"John\", \"Dr. Emily\", \"Professor David\"]\n",
    "    last_names = [\"Chen\", \"Patel\", \"Johnson\", \"Lee\", \"Gomez\", \"Smith\"]\n",
    "    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
    "\n",
    "def generate_company_name():\n",
    "    prefixes = [\"Tech\", \"Quantum\", \"Cyber\", \"Data\", \"Innovate\", \"Future\"]\n",
    "    suffixes = [\"Labs\", \"Corp\", \"Inc\", \"Technologies\", \"Group\", \"Solutions\"]\n",
    "    return f\"{random.choice(prefixes)} {random.choice(suffixes)}\"\n",
    "\n",
    "# Generate headline\n",
    "def generate_headline(model, tokenizer, topic, sentiment, article_type):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prompt = f\"Write a {sentiment} headline for a {article_type} article about {topic} in 2025. Example: 'AI Tool Boosts Efficiency by 30% in Manufacturing'.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        do_sample=False,  # Greedy decoding for coherence\n",
    "        temperature=0.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    headline = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "    words = headline.split()\n",
    "    if 5 <= len(words) <= 15:  # Reasonable headline length\n",
    "        return headline\n",
    "    return None\n",
    "\n",
    "# Generate article body\n",
    "def generate_article_body(model, tokenizer, headline, topic, sentiment, article_type, expert_name, company_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prompt = f\"\"\"\n",
    "Write a {sentiment} {article_type} article (250-500 words) about {topic} in 2025. Include:\n",
    "- Intro: A key event in 2025 involving {company_name}.\n",
    "- Details: Use dates in 2025 and product specifics.\n",
    "- Quote: From {expert_name}.\n",
    "- Conclusion: Future implications.\n",
    "Example: \"In 2025, Tech Labs unveiled Synthia, an AI tool boosting efficiency by 30%. 'A game-changer,' said Dr. John Lee.\"\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=500,\n",
    "        do_sample=False,  # Greedy decoding for coherence\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    article_body = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "    return article_body\n",
    "\n",
    "# Validate article\n",
    "def validate_article(article_body):\n",
    "    word_count = len(article_body.split())\n",
    "    has_quote = '\"' in article_body\n",
    "    return 250 <= word_count <= 500 and has_quote\n",
    "\n",
    "# Generate dataset\n",
    "def generate_dataset(num_articles=10):\n",
    "    tokenizer, model = load_model()\n",
    "    articles = []\n",
    "    for _ in range(num_articles):\n",
    "        topic = random.choice(topics)\n",
    "        sentiment = random.choice(sentiments)\n",
    "        article_type = random.choice(article_types)\n",
    "        expert_name = generate_expert_name()\n",
    "        company_name = generate_company_name()\n",
    "\n",
    "        # Generate headline with retry\n",
    "        headline = None\n",
    "        for _ in range(3):\n",
    "            headline = generate_headline(model, tokenizer, topic, sentiment, article_type)\n",
    "            if headline:\n",
    "                break\n",
    "        if not headline:\n",
    "            print(f\"Failed to generate headline for {topic}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Generate article body with retry\n",
    "        article_body = None\n",
    "        for _ in range(3):\n",
    "            article_body = generate_article_body(model, tokenizer, headline, topic, sentiment, article_type, expert_name, company_name)\n",
    "            if validate_article(article_body):\n",
    "                articles.append([headline, article_body, topic])\n",
    "                print(f\"Generated article for {topic} with {sentiment} sentiment.\")\n",
    "                break\n",
    "        if not article_body or not validate_article(article_body):\n",
    "            print(f\"Failed to generate valid article for {topic}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    # Save to CSV\n",
    "    with open('dataset-gpt2-highertemp.csv', 'w', newline='', encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Headline', 'Article Body', 'Topic'])\n",
    "        writer.writerows(articles)\n",
    "    print(\"Dataset generation complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eli-YsxHkMv6"
   },
   "source": [
    "# GPT2 Revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JldLYW-kPja",
    "outputId": "fe7b3288-af38-4885-d6d6-7c02feeaf2f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated article for Augmented/Virtual Reality (AR/VR) with negative sentiment.\n",
      "Generated article for Artificial Intelligence (AI) with negative sentiment.\n",
      "Generated article for Artificial Intelligence (AI) with negative sentiment.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import random\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define examples\n",
    "example_positive = \"\"\"\n",
    "Headline: AI Tool Boosts Efficiency by 30% in Manufacturing\n",
    "Introduction: In 2025, Tech Labs announced a new AI tool called Synthia.\n",
    "Details: Synthia uses advanced algorithms to optimize production lines, launched on June 1, 2025.\n",
    "Quote: \"This is a game-changer for the industry,\" said Dr. John Lee.\n",
    "Conclusion: This development could lead to widespread adoption of AI in manufacturing.\n",
    "\"\"\"\n",
    "\n",
    "example_negative = \"\"\"\n",
    "Headline: Robotics Industry Faces Ethical Dilemma Over Job Displacement\n",
    "Introduction: In 2025, a report highlighted concerns about robotics replacing human jobs.\n",
    "Details: The report, published by Future Group on July 15, 2025, cites a 25% increase in unemployment in certain sectors.\n",
    "Quote: \"We need to address the social impact of automation,\" said Professor Maria Smith.\n",
    "Conclusion: The industry must find ways to balance innovation with social responsibility.\n",
    "\"\"\"\n",
    "\n",
    "def generate_article(model, tokenizer, topic, sentiment, article_type, expert_name, company_name):\n",
    "    prompt = f\"\"\"\n",
    "Below are examples of news articles:\n",
    "\n",
    "{example_positive}\n",
    "\n",
    "{example_negative}\n",
    "\n",
    "Now, write a {sentiment} {article_type} article about {topic} in 2025 in the same format.\n",
    "\n",
    "Headline:\n",
    "Introduction:\n",
    "Details:\n",
    "Quote:\n",
    "Conclusion:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=1000,\n",
    "        do_sample=False,\n",
    "        temperature=0.1,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    article_start = generated_text.find(\"Now, write a\")\n",
    "    if article_start != -1:\n",
    "        generated_text = generated_text[article_start:]\n",
    "\n",
    "    sections = {\n",
    "        \"Headline\": re.search(r\"Headline:\\s*(.*)\", generated_text),\n",
    "        \"Introduction\": re.search(r\"Introduction:\\s*(.*?)Details:\", generated_text, re.DOTALL),\n",
    "        \"Details\": re.search(r\"Details:\\s*(.*?)Quote:\", generated_text, re.DOTALL),\n",
    "        \"Quote\": re.search(r\"Quote:\\s*(.*?)Conclusion:\", generated_text, re.DOTALL),\n",
    "        \"Conclusion\": re.search(r\"Conclusion:\\s*(.*)\", generated_text, re.DOTALL)\n",
    "    }\n",
    "\n",
    "    if all(sections[key] for key in sections):\n",
    "        headline = sections[\"Headline\"].group(1).strip()\n",
    "        article_body = f\"{sections['Introduction'].group(1).strip()}\\n\\n{sections['Details'].group(1).strip()}\\n\\n{sections['Quote'].group(1).strip()}\\n\\n{sections['Conclusion'].group(1).strip()}\"\n",
    "        article_body = article_body.replace(\"[Company]\", company_name).replace(\"[Expert]\", expert_name)\n",
    "        return headline, article_body\n",
    "    return None, None\n",
    "\n",
    "# Example usage\n",
    "topics = [\"Artificial Intelligence (AI)\", \"Augmented/Virtual Reality (AR/VR)\"]\n",
    "sentiments = [\"positive\", \"negative\"]\n",
    "article_types = [\"breakthrough announcement\", \"ethical debate\"]\n",
    "articles = []\n",
    "\n",
    "for _ in range(3):  # Generate 3 articles\n",
    "    topic = random.choice(topics)\n",
    "    sentiment = random.choice(sentiments)\n",
    "    article_type = random.choice(article_types)\n",
    "    expert_name = \"Dr. Jane Doe\"  # Replace with your name generator\n",
    "    company_name = \"InnovateCorp\"  # Replace with your company generator\n",
    "\n",
    "    for attempt in range(3):\n",
    "        headline, article_body = generate_article(model, tokenizer, topic, sentiment, article_type, expert_name, company_name)\n",
    "        if headline and article_body:\n",
    "            articles.append([headline, article_body, topic])\n",
    "            print(f\"Generated article for {topic} with {sentiment} sentiment.\")\n",
    "            break\n",
    "        print(f\"Attempt {attempt+1} failed for {topic}. Retrying...\")\n",
    "    else:\n",
    "        print(f\"Failed to generate article for {topic} after 3 attempts.\")\n",
    "\n",
    "# Save to CSV (example)\n",
    "import csv\n",
    "with open(\"dataset-gpt2-newapproach.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Headline\", \"Article Body\", \"Topic\"])\n",
    "    writer.writerows(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNzvqfntnMrc"
   },
   "source": [
    "# Flan T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411,
     "referenced_widgets": [
      "4a92b774a9304d15a48bb90874a82dc1",
      "db4d47478f1a419890eac09ef879b944",
      "d8648bb2d76246dabe819461caba8288",
      "34591dad4c6f4f48ac0d6302c942cba8",
      "46ae0186d28b4094bebb57be1cf2669b",
      "cd13fbbc4a494e6dae8efedc7cacbbe6",
      "56722b3c4114403cbb06f072db8aaa90",
      "f0a1ef8627764451af815573c480906b",
      "b77f59148ebf4428975e45d9f1e79c55",
      "18a92f1abc244d7daf3a72885339bd38",
      "8d455f56e6d248ff93da241683437440"
     ]
    },
    "id": "GsHQ6LrmnPrT",
    "outputId": "5dae8b86-2d7b-4a5d-eb7d-9389feb290cb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a92b774a9304d15a48bb90874a82dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n",
      "Generating headline...\n",
      "Generating article body...\n",
      "Saved article: Cybersecurity in 2025\n",
      "Generating headline...\n",
      "Generating article body...\n",
      "Saved article: Ethical debate: 3D Printing in 2025\n",
      "Generating headline...\n",
      "Generating article body...\n",
      "Saved article: Artificial Intelligence Will Not Be Ready By 2025\n",
      "Generating headline...\n",
      "Generating article body...\n",
      "Saved article: Autonomous Vehicles Will Not Be Ready By 2025\n",
      "Generating headline...\n",
      "Generating article body...\n",
      "Saved article: Quantum computing to be a major market by 2025\n",
      "Generating headline...\n",
      "Generating article body...\n",
      "Saved article: Cloud Computing in 2025: 'It's not going to happen'\n",
      "Generating headline...\n",
      "Generating article body...\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "def load_model():\n",
    "    model_name = \"google/flan-t5-xl\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "    print(\"Model and tokenizer loaded.\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# Define parameters\n",
    "topics = [\"Artificial Intelligence (AI)\", \"Robotics\", \"Blockchain\", \"Augmented/Virtual Reality (AR/VR)\",\n",
    "          \"Cybersecurity\", \"Quantum Computing\", \"5G Technology\", \"Autonomous Vehicles\",\n",
    "          \"Cloud Computing\", \"3D Printing\"]\n",
    "sentiments = [\"positive\", \"negative\"]\n",
    "article_types = [\"breakthrough announcement\", \"market analysis\", \"ethical debate\", \"product launch\"]\n",
    "\n",
    "def generate_expert_name():\n",
    "    first_names = [\"Dr. Alan\", \"Professor Maria\", \"Sarah\", \"John\", \"Dr. Emily\", \"Professor David\"]\n",
    "    last_names = [\"Chen\", \"Patel\", \"Johnson\", \"Lee\", \"Gomez\", \"Smith\"]\n",
    "    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
    "\n",
    "def generate_company_name():\n",
    "    prefixes = [\"Tech\", \"Quantum\", \"Cyber\", \"Data\", \"Innovate\", \"Future\"]\n",
    "    suffixes = [\"Labs\", \"Corp\", \"Inc\", \"Technologies\", \"Group\", \"Solutions\"]\n",
    "    return f\"{random.choice(prefixes)} {random.choice(suffixes)}\"\n",
    "\n",
    "def generate_headline(model, tokenizer, topic, sentiment, article_type):\n",
    "    print('Generating headline...')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prompt = f\"Generate a {sentiment} headline (10-15 words) for a {article_type} article about {topic} in 2025.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "ef generate_article_body(model, tokenizer, headline, topic, sentiment, article_type, expert_name, company_name, retry_count):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    random_day = random.randint(1, 28)\n",
    "    random_month = random.choice([\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"])\n",
    "    random_date = f\"{random_month} {random_day}, 2025\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Write a professional {article_type} article on {topic} with a {sentiment} sentiment. Follow these guidelines:\n",
    "    - Headline: {headline}\n",
    "    - Length: 250-500 words, 4-8 paragraphs.\n",
    "    - Structure:\n",
    "      - Paragraph 1: Introduction summarizing a key event or trend in {random_date}.\n",
    "      - Paragraph 2-5: Detailed analysis, including {company_name}'s role and impact.\n",
    "      - At least one quote from {expert_name} reinforcing the {sentiment} sentiment.\n",
    "      - Use optional statistics (e.g., \"a 30% increase\") to enhance realism.\n",
    "      - Conclusion on future outlook.\n",
    "    - Style: Mimic authentic technology journalism. Avoid first-person narration or model self-explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Adaptive instructions based on retry count\n",
    "    if retry_count > 0:\n",
    "        if retry_count == 1:\n",
    "            prompt += \" Ensure the article is at least 250 words long.\"\n",
    "        elif retry_count == 2:\n",
    "            prompt += f\" Include a quote from {expert_name} about {topic}.\"\n",
    "        elif retry_count >= 3:\n",
    "            prompt += \" Avoid repeating information.\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=600,\n",
    "        num_beams=2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    article_body = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    generation_time = time.time() - start_time\n",
    "\n",
    "    word_count = len(article_body.split())\n",
    "    has_quote = '\"' in article_body\n",
    "    print(f\"Retry {retry_count}: Generated in {generation_time:.2f} seconds, Words: {word_count}, Quote: {has_quote}\")\n",
    "    print(f\"Prompt: {prompt[:100]}...\")\n",
    "\n",
    "    if 250 <= word_count <= 500 and has_quote:\n",
    "        return article_body\n",
    "    return None\n",
    "\n",
    "def generate_dataset(num_articles=5):\n",
    "    tokenizer, model = load_model()\n",
    "    file_path = 'dataset-flan-t5xl-dynamicpromptfeedback.csv'\n",
    "\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    if not file_exists:\n",
    "        with open(file_path, 'w', newline='', encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Headline', 'Article Body', 'Topic'])\n",
    "\n",
    "    for _ in range(num_articles):\n",
    "        topic = random.choice(topics)\n",
    "        sentiment = random.choice(sentiments)\n",
    "        article_type = random.choice(article_types)\n",
    "        expert_name = generate_expert_name()\n",
    "        company_name = generate_company_name()\n",
    "\n",
    "        headline = generate_headline(model, tokenizer, topic, sentiment, article_type)\n",
    "        if not headline:\n",
    "            print(f\"Failed to generate a valid headline for {topic}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        article_body = None\n",
    "        for _ in range(5):  # Try multiple times for a valid article\n",
    "            article_body = generate_article_body(model, tokenizer, headline, topic, sentiment, article_type, expert_name, company_name)\n",
    "            if article_body:\n",
    "                break\n",
    "\n",
    "        if not article_body:\n",
    "            print(f\"Failed to generate valid article for {topic}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, 'a', newline='', encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([headline, article_body, topic])\n",
    "\n",
    "        print(f\"Saved article: {headline}\")\n",
    "\n",
    "    print(\"Dataset generation complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN19qTB6QvhL"
   },
   "source": [
    "# Updated FLAN-T5 with enhanced validation\n",
    "\n",
    "### **Enhancements in This Version**\n",
    "- **Stricter Headline and Article Validation**  \n",
    "  - Enforced sentiment alignment using `VADER` (`polarity > 0.1` for positive tone).  \n",
    "  - Ensured topic relevance via keyword matching and cosine similarity (`> 0.7`).  \n",
    "  - Implemented uniqueness checks using Levenshtein distance (`similarity < 0.8`).  \n",
    "  - Set headline length constraints (`10-15 words`, title case enforced).  \n",
    "\n",
    "- **Article Structure and Quality Improvements**  \n",
    "  - Verified presence of key sections (`Introduction`, `Details`, `Quote`, `Conclusion`).  \n",
    "  - Enforced paragraph count (`4-8` paragraphs) and statistical inclusion (`\\d+%|\\d{4}`).  \n",
    "  - Added Named Entity Recognition (NER) for expert and company mentions (`SpaCy`).  \n",
    "  - Ensured proper attribution for quotes (`\"said [expert]\"`).  \n",
    "  - Enforced Flesch reading ease (`50-60`) for readability.  \n",
    "  - Checked for sentence repetition using cosine similarity (`< 0.9`).  \n",
    "\n",
    "- **Performance and Robustness Enhancements**  \n",
    "  - Retried article generation up to **5 times** with adaptive prompts (e.g., enforcing quote inclusion, avoiding repetition).  \n",
    "  - Expanded **logging visibility** for retry attempts, generation times, validation failures, and dataset progress.  \n",
    "  - Optimized **tokenizer/model execution** to minimize compute overhead (`num_beams=3`, `no_repeat_ngram_size=3`).  \n",
    "\n",
    "- **File Handling & Dataset Management**  \n",
    "  - Ensured **consistent dataset appending** with CSV validation.  \n",
    "  - Logged **failed attempts** for better debugging insights.  \n",
    "  - Maintained structured format for reproducibility.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wVbNP_CRAxt"
   },
   "outputs": [],
   "source": [
    "  topics = [\"Artificial Intelligence (AI)\", \"Robotics\", \"Blockchain\", \"Augmented/Virtual Reality (AR/VR)\",\n",
    "          \"Cybersecurity\", \"Quantum Computing\", \"5G Technology\", \"Autonomous Vehicles\",\n",
    "          \"Cloud Computing\", \"3D Printing\"]\n",
    "sentiments = [\"positive\", \"negative\"]\n",
    "article_types = [\"breakthrough announcement\", \"market analysis\", \"ethical debate\", \"product launch\",\n",
    "                 \"funding round\", \"acquisition deal\", \"macroeconomic conditions\", \"research development\", \"opinion piece\"]\n",
    "\n",
    "# Load sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Track failed articles\n",
    "failed_articles = []\n",
    "\n",
    "def generate_expert_name():\n",
    "    first_names = [\"Dr. Alan\", \"Professor Maria\", \"Sarah\", \"John\", \"Dr. Emily\", \"Professor David\"]\n",
    "    last_names = [\"Chen\", \"Patel\", \"Johnson\", \"Lee\", \"Gomez\", \"Smith\"]\n",
    "    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
    "\n",
    "\n",
    "def generate_company_name():\n",
    "    prefixes = [\"Tech\", \"Quantum\", \"Cyber\", \"Data\", \"Innovate\", \"Future\"]\n",
    "    suffixes = [\"Labs\", \"Corp\", \"Inc\", \"Technologies\", \"Group\", \"Solutions\"]\n",
    "    return f\"{random.choice(prefixes)} {random.choice(suffixes)}\"\n",
    "\n",
    "\n",
    "def generate_headline(model, tokenizer, topic, sentiment, article_type, feedback=\"\"):\n",
    "    print(f'Generating headline (feedback was: {feedback})...')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dynamic feedback injection\n",
    "    prompt = f\"\"\"\n",
    "    As a professional journalist, generate a descriptive, {sentiment} headline (15-25 words)\n",
    "    for a {article_type} article about {topic}. It should be current (we are in 2025), and can\n",
    "    make reference to the date, but does not have to. {feedback}\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=75,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.9,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    headline = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Validate headline length\n",
    "    word_count = len(headline.split())\n",
    "    sentiment_score = analyzer.polarity_scores(headline)['compound']\n",
    "\n",
    "    print(f\"Generated headline sentiment score: {sentiment_score}\")\n",
    "\n",
    "    # Adjustments based on failures\n",
    "    feedback = \"\"\n",
    "    if word_count < 6:\n",
    "        feedback += \" Ensure the headline is at least 15 words long.\"\n",
    "    if (sentiment == \"positive\" and sentiment_score < 0.05) or \\\n",
    "       (sentiment == \"negative\" and sentiment_score > -0.05):\n",
    "        feedback += f\" The sentiment should be more clearly {sentiment}.\"\n",
    "\n",
    "    if feedback:\n",
    "        print(f\"Headline sentiment misaligned or too short: {headline}\")\n",
    "        return None, feedback\n",
    "\n",
    "    return headline, \"\"\n",
    "\n",
    "\n",
    "def generate_article_body(model, tokenizer, headline, topic, sentiment, article_type, expert_name, company_name, retry_count=0, feedback=\"\"):\n",
    "    print(f'Generating article body (attempt {retry_count + 1}, feedback was: {feedback})...')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    random_day = random.randint(1, 28)\n",
    "    random_month = random.choice([\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"])\n",
    "    random_date = f\"{random_month} {random_day}, 2025\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Write a professional {article_type} article on {topic} with a {sentiment} sentiment. Follow these guidelines:\n",
    "    - Headline: {headline}\n",
    "    - Length: 350-500 words, 4-8 paragraphs.\n",
    "    - Structure:\n",
    "      - Introduction summarizing a key event or trend in {random_date}.\n",
    "      - Detailed analysis, including {company_name}'s role and impact.\n",
    "      - At least one quote from {expert_name} reinforcing the {sentiment} sentiment.\n",
    "      - Use optional statistics (e.g., \"a 30% increase\") to enhance realism.\n",
    "      - Conclusion on future outlook.\n",
    "    - Style: Mimic authentic technology journalism. Aim for Flesch Reading Ease score between 50-60.\n",
    "    {feedback}\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=700,\n",
    "        min_length=400,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    article_body = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    generation_time = time.time() - start_time\n",
    "\n",
    "    # Validate article\n",
    "    word_count = len(article_body.split())\n",
    "    has_quote = '\"' in article_body\n",
    "    readability_score = textstat.flesch_reading_ease(article_body)\n",
    "\n",
    "    print(f\"Retry {retry_count}: Generated in {generation_time:.2f} seconds, Words: {word_count}, Quote: {has_quote}, Readability: {readability_score:.2f}\")\n",
    "\n",
    "    feedback = \"\"\n",
    "    if word_count < 250 or word_count > 500:\n",
    "        feedback += \" Adjust the length to be within 350-500 words. This is important.\"\n",
    "    if not has_quote:\n",
    "        feedback += \" Include at least one expert quote. This is important.\"\n",
    "    if readability_score < 50 or readability_score > 60:\n",
    "        feedback += \" Adjust readability to be between 50-60. This is important.\"\n",
    "\n",
    "    if not feedback:\n",
    "        return article_body, \"\"\n",
    "\n",
    "    print(f\"Article validation failed: {article_body[:250]}\\nFeedback: {feedback}\")\n",
    "    return None, feedback\n",
    "\n",
    "\n",
    "def generate_dataset(num_articles=5):\n",
    "    tokenizer, model = load_model()\n",
    "    file_path = 'dataset-flan-t5xl-dynamicpromptfeedback-take2.csv'\n",
    "    failed_articles_path = 'failed_dataset-flan-t5xl-dynamicpromptfeedback.csv'\n",
    "\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    if not file_exists:\n",
    "        with open(file_path, 'w', newline='', encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Headline', 'Article Body', 'Topic'])\n",
    "\n",
    "    for _ in range(num_articles):\n",
    "        topic = random.choice(topics)\n",
    "        sentiment = random.choice(sentiments)\n",
    "        article_type = random.choice(article_types)\n",
    "        expert_name = generate_expert_name()\n",
    "        company_name = generate_company_name()\n",
    "        feedback = \"\"\n",
    "\n",
    "        for retry in range(10):\n",
    "            headline, feedback = generate_headline(model, tokenizer, topic, sentiment, article_type, feedback)\n",
    "            if headline:\n",
    "                print(f\"Generated headline: {headline}\")\n",
    "                break\n",
    "            print(f\"Retry {retry}: Failed to generate a valid headline for {topic}.\")\n",
    "\n",
    "        if not headline:\n",
    "            print(f\"Failed to generate valid headline for {topic}. Skipping.\")\n",
    "            failed_articles.append((topic, sentiment, article_type, \"No valid headline\"))\n",
    "            continue\n",
    "\n",
    "        article_body = None\n",
    "        for retry in range(5):\n",
    "            article_body, feedback = generate_article_body(model, tokenizer, headline, topic, sentiment, article_type, expert_name, company_name, retry, feedback)\n",
    "            if article_body:\n",
    "                break\n",
    "\n",
    "        if not article_body:\n",
    "            print(f\"Failed to generate valid article for {topic}. Skipping.\")\n",
    "            failed_articles.append((topic, sentiment, article_type, headline))\n",
    "            continue\n",
    "\n",
    "        with open(file_path, 'a', newline='', encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([headline, article_body, topic])\n",
    "\n",
    "        print(f\"Saved article: {headline}\")\n",
    "\n",
    "    # Save failed articles\n",
    "    with open(failed_articles_path, 'w', newline='', encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Topic', 'Sentiment', 'Article Type', 'Failed Headline'])\n",
    "        writer.writerows(failed_articles)\n",
    "\n",
    "    print(\"Dataset generation complete. Failed articles saved for review.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WlHzEeK9eqaq",
    "outputId": "7a82f41d-cafd-4015-ee0b-d251b0f19be6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795fc58c156b4476b4112665c62f4d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba69bf60c7a4fe58a70df94296109e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dca95c5a71489d9bddca1f1838a1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e96322e46ee40c2aad1629a4b9ee27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5781018f3a4542bc55ade6429b40ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22993de8e48a42599f955e49157cea95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d5c2024df44702852310fec67814a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b147de8a6504e0f96d4959cb37ae7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac2792ca86a440a816c3def211d04e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4def4311e57b4c45b5f334ea90199a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35ee9b09362471eb3d6ba8a1c67175f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded.\n",
      "Generating headline (feedback was: )...\n",
      "Generated headline sentiment score: 0.3612\n",
      "Generated headline: IBM's Watson AI 'thinks' like a human\n",
      "Generating article body (attempt 1, feedback was: )...\n",
      "Retry 0: Generated in 854.47 seconds, Words: 309, Quote: True, Readability: 61.46\n",
      "Article validation failed: IBM's Watson AI 'thinks' like a human, according to Dr. Emily Johnson, a cognitive scientist at the Data Group. \"It's the first time that a computer has been able to think in the same way as a person,\" she said in a statement. \"We've been working on \n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 2, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 1: Generated in 869.84 seconds, Words: 289, Quote: True, Readability: 60.61\n",
      "Article validation failed: IBM's Watson AI 'thinks' like a human - Length: 350-500 words, 4-8 paragraphs. Structure: Introduction summarizing a key event or trend in February 6, 2025. Detailed analysis, including Data Group's role and impact. At least one quote from Dr. Emily \n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 3, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 2: Generated in 847.79 seconds, Words: 252, Quote: True, Readability: 42.98\n",
      "Article validation failed: IBM's Watson AI 'thinks' like a human - Length: 350-500 words, 4-8 paragraphs. Structure: Introduction summarizing a key event or trend in August 7, 2025. Detailed analysis, including Data Group's role and impact. At least one quote from Dr. Emily Jo\n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 4, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 3: Generated in 882.73 seconds, Words: 256, Quote: True, Readability: 52.26\n",
      "Saved article: IBM's Watson AI 'thinks' like a human\n",
      "Generating headline (feedback was: )...\n",
      "Generated headline sentiment score: 0.0\n",
      "Headline sentiment misaligned or too short: Quantum computing: IBM's 'supercomputer'\n",
      "Retry 0: Failed to generate a valid headline for Quantum Computing.\n",
      "Generating headline (feedback was:  Ensure the headline is at least 15 words long. The sentiment should be more clearly positive.)...\n",
      "Generated headline sentiment score: 0.0\n",
      "Headline sentiment misaligned or too short: Quantum computing: IBM's new chip uses atoms to speed up calculations\n",
      "Retry 1: Failed to generate a valid headline for Quantum Computing.\n",
      "Generating headline (feedback was:  The sentiment should be more clearly positive.)...\n",
      "Generated headline sentiment score: 0.2975\n",
      "Generated headline: IBM's Quantum Chip Is Almost Ready\n",
      "Generating article body (attempt 1, feedback was: )...\n",
      "Retry 0: Generated in 926.15 seconds, Words: 294, Quote: True, Readability: 50.26\n",
      "Saved article: IBM's Quantum Chip Is Almost Ready\n",
      "Generating headline (feedback was: )...\n",
      "Generated headline sentiment score: -0.1531\n",
      "Headline sentiment misaligned or too short: Quantum computing: will it be good or bad for humanity?\n",
      "Retry 0: Failed to generate a valid headline for Quantum Computing.\n",
      "Generating headline (feedback was:  The sentiment should be more clearly positive.)...\n",
      "Generated headline sentiment score: 0.0\n",
      "Headline sentiment misaligned or too short: Quantum computing: the future of computing?\n",
      "Retry 1: Failed to generate a valid headline for Quantum Computing.\n",
      "Generating headline (feedback was:  The sentiment should be more clearly positive.)...\n",
      "Generated headline sentiment score: -0.296\n",
      "Headline sentiment misaligned or too short: Quantum computing: Should we be worried?\n",
      "Retry 2: Failed to generate a valid headline for Quantum Computing.\n",
      "Generating headline (feedback was:  The sentiment should be more clearly positive.)...\n",
      "Generated headline sentiment score: 0.4404\n",
      "Generated headline: Quantum computing: 'It's a good thing'\n",
      "Generating article body (attempt 1, feedback was: )...\n",
      "Retry 0: Generated in 945.61 seconds, Words: 310, Quote: True, Readability: 42.21\n",
      "Article validation failed: Quantum computing: 'It's a good thing' - Professor Maria Lee, Innovate Technologies - November 17, 2025. Professor Lee, a theoretical physicist at the University of California, Berkeley, is a leading expert on quantum computing. She has written exten\n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 2, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 1: Generated in 937.25 seconds, Words: 298, Quote: True, Readability: 47.89\n",
      "Article validation failed: Quantum computing: 'It's a good thing' is a professional ethical debate article on Quantum Computing with a positive sentiment. It should seem plausibly written by a prominent national/international newspaper or media outlet. Aim for Flesch Reading E\n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 3, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 2: Generated in 934.92 seconds, Words: 291, Quote: True, Readability: 47.49\n",
      "Article validation failed: Quantum computing: 'It's a good thing' is a professional ethical debate article on Quantum Computing with a positive sentiment. It should seem plausibly written by a prominent national/international newspaper or media outlet. Aim for Flesch Reading E\n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 4, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 3: Generated in 907.22 seconds, Words: 276, Quote: True, Readability: 50.84\n",
      "Saved article: Quantum computing: 'It's a good thing'\n",
      "Generating headline (feedback was: )...\n",
      "Generated headline sentiment score: 0.3818\n",
      "Generated headline: Cybersecurity to be a big growth area in 2025\n",
      "Generating article body (attempt 1, feedback was: )...\n",
      "Retry 0: Generated in 963.22 seconds, Words: 280, Quote: True, Readability: 38.72\n",
      "Article validation failed: Cybersecurity will be a big growth area in 2025, according to John Patel, founder and CEO of Cyber Technologies, a cybersecurity company based in New York City. Patel predicted that the cybersecurity industry will grow at a compound annual growth rat\n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 2, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 1: Generated in 887.73 seconds, Words: 295, Quote: True, Readability: 44.34\n",
      "Article validation failed: Cybersecurity will be a big growth area in 2025, according to John Patel, CEO of Cyber Technologies, a cybersecurity company based in New York City. Patel predicts that the cybersecurity industry will grow at a compound annual growth rate (CAGR) of 1\n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 3, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 2: Generated in 1027.38 seconds, Words: 253, Quote: True, Readability: 45.86\n",
      "Article validation failed: Cybersecurity will be a big growth area in 2025, according to a new report from John Patel. The report, titled \"Cybersecurity to Be a Big Growth Area by 2025,\" predicts that the cybersecurity industry will grow at a compound annual growth rate (CAGR)\n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 4, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 3: Generated in 945.43 seconds, Words: 311, Quote: True, Readability: 45.46\n",
      "Article validation failed: Cybersecurity will be a big growth area in 2025, according to John Patel, CEO of Cyber Technologies, a cybersecurity company based in Palo Alto, California. According to Patel's research, the cybersecurity industry will grow at a compound annual grow\n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 5, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 4: Generated in 987.90 seconds, Words: 305, Quote: False, Readability: 45.86\n",
      "Article validation failed: Cybersecurity will be a big growth area in 2025, according to John Patel, CEO and founder of Cyber Technologies. Patel predicts that cybersecurity will be one of the fastest growing areas in the economy by 2025. He predicts a 30% increase in the cybe\n",
      "Feedback:  Include at least one expert quote. This is important. Adjust readability to be between 50-60. This is important.\n",
      "Failed to generate valid article for Cybersecurity. Skipping.\n",
      "Generating headline (feedback was: )...\n",
      "Generated headline sentiment score: 0.0\n",
      "Headline sentiment misaligned or too short: Samsung's 5G phone is a bust\n",
      "Retry 0: Failed to generate a valid headline for 5G Technology.\n",
      "Generating headline (feedback was:  The sentiment should be more clearly negative.)...\n",
      "Generated headline sentiment score: -0.2755\n",
      "Headline sentiment misaligned or too short: 5G technology isn't ready yet\n",
      "Retry 1: Failed to generate a valid headline for 5G Technology.\n",
      "Generating headline (feedback was:  Ensure the headline is at least 15 words long.)...\n",
      "Generated headline sentiment score: -0.2755\n",
      "Generated headline: 5G 'not ready for prime time'\n",
      "Generating article body (attempt 1, feedback was: )...\n",
      "Retry 0: Generated in 1086.99 seconds, Words: 305, Quote: True, Readability: 71.24\n",
      "Article validation failed: 5G technology is not ready for prime time, according to Professor Maria Chen of the University of California, Berkeley. Chen, a leading expert in the field of 5G, said that the technology is still in its infancy and will not be ready for commercial u\n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 2, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 1: Generated in 954.37 seconds, Words: 287, Quote: True, Readability: 43.63\n",
      "Article validation failed: 5G Technology is not ready for prime time, according to Professor Maria Chen, a leading expert on 5G technology. Chen, who is a professor at the University of California, Berkeley, said that 5G will not be ready for commercial use by 2025. \"It's not \n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 3, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 2: Generated in 963.68 seconds, Words: 312, Quote: True, Readability: 71.75\n",
      "Article validation failed: 5G Technology is not ready for prime time, according to Professor Maria Chen, a leading expert on 5G technology. Chen, who is a professor at the University of California, Berkeley, said that 5G will not be ready for commercial use by 2025. \"It's not \n",
      "Feedback:  Adjust readability to be between 50-60. This is important.\n",
      "Generating article body (attempt 4, feedback was:  Adjust readability to be between 50-60. This is important.)...\n",
      "Retry 3: Generated in 1046.65 seconds, Words: 313, Quote: True, Readability: 50.26\n",
      "Saved article: 5G 'not ready for prime time'\n",
      "Dataset generation complete. Failed articles saved for review.\n"
     ]
    }
   ],
   "source": [
    "generate_dataset()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNINhqTRsb4YHreOjw45src",
   "collapsed_sections": [
    "V6Fr8h5QZziy",
    "lA0g2Hq7cQsw",
    "0bjHgEw5hV3T",
    "Eli-YsxHkMv6"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
